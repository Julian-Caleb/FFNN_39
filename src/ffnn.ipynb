{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar 1 IF3270 Pembelajaran Mesin <br /> Feedforward Neural Network\n",
    "\n",
    "## Kelompok 39\n",
    "\n",
    "- Dzaky Satrio Nugroho - 13522059\n",
    "- Julian Caleb Simandjuntak - 13522099\n",
    "- Rafiki Prawhira Harianto - 13522065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dulu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Aktivasi \n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self, activation_type):\n",
    "        self.activation_type = activation_type\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self.activation_type == 'linear':\n",
    "            return ActivationFunction.__linear(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return ActivationFunction.__relu(x)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            return ActivationFunction.__sigmoid(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return ActivationFunction.__tanh(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return ActivationFunction.__softmax(x)\n",
    "        elif self.activation_type == 'leaky_relu':\n",
    "            return ActivationFunction.__leaky_relu(x)\n",
    "        elif self.activation_type == 'swish':\n",
    "            return ActivationFunction.__swish(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{self.activation_type}' not supported\")\n",
    "        \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self.activation_type == 'linear':\n",
    "            return ActivationFunction.__linear_derivative(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return ActivationFunction.__relu_derivative(x)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            return ActivationFunction.__sigmoid_derivative(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return ActivationFunction.__tanh_derivative(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return ActivationFunction.__softmax_derivative(x)\n",
    "        elif self.activation_type == 'leaky_relu':\n",
    "            return ActivationFunction.__leaky_relu_derivative(x)\n",
    "        elif self.activation_type == 'swish':\n",
    "            return ActivationFunction.__swish_derivative(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{self.activation_type}' not supported\")\n",
    "    \n",
    "    def __linear(x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    def __relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def __sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __softmax(x: np.ndarray) -> np.ndarray:\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def __leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.maximum(alpha*x, x)\n",
    "\n",
    "    def __swish(x: np.ndarray) -> np.ndarray:\n",
    "        return x * ActivationFunction.__sigmoid(x)\n",
    "\n",
    "    def __linear_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    def __relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def __sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return (2 / (2 * np.sinh(x))) ** 2\n",
    "\n",
    "    def __softmax_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        softmaxx = ActivationFunction.softmax(x)\n",
    "        n = x.size\n",
    "        matrix = []\n",
    "        for i in range(1,n+1):\n",
    "            row = []\n",
    "            for j in range(1,n+1):\n",
    "                row.append(softmaxx[i-1] * ((i == j) - softmaxx[j-1]))\n",
    "            matrix.append(row)\n",
    "\n",
    "        return np.array(matrix)\n",
    "\n",
    "    def __leaky_relu_derivative(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    def __swish_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.sigmoid(x)\n",
    "        return sigmoidx * (1 + x - x * sigmoidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Loss\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, loss_type: str):\n",
    "        self.loss_type = loss_type\n",
    "    \n",
    "    def loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        if self.loss_type == 'mse':\n",
    "            return LossFunction.__mse(y_pred, y_true)\n",
    "        elif self.loss_type == 'bce':\n",
    "            return LossFunction.__bce(y_pred, y_true)\n",
    "        elif self.loss_type == 'cce':\n",
    "            return LossFunction.__cce(y_pred, y_true)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function {self.loss_type}\")\n",
    "        \n",
    "    def loss_derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        if self.loss_type == 'mse':\n",
    "            return LossFunction.__mse_derivative(y_pred, y_true)\n",
    "        elif self.loss_type == 'bce':\n",
    "            return LossFunction.__bce_derivative(y_pred, y_true)\n",
    "        elif self.loss_type == 'cce':\n",
    "            return LossFunction.__cce_derivative(y_pred, y_true)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function {self.loss_type}\")\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    def __mse(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        mse = np.sum((y_true - y_pred) ** 2) / len(y_true)\n",
    "        return mse\n",
    "\n",
    "    # Binary Cross-Entropy\n",
    "    def __bce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        bce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "        return bce\n",
    "\n",
    "    # Categorical Cross-Entropy\n",
    "    def __cce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        cce = -1 / len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
    "        return cce\n",
    "    \n",
    "    def __mse_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -2 * (y_true - y_pred) / len(y_true) # times dy_pred/dw \n",
    "    \n",
    "    def __bce_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -1 * (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true)) # times dy_pred/dw \n",
    "    \n",
    "    def __cce_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -1 * (y_true / (y_pred * len(y_true))) # times dy_pred/dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Inisialisasi 1 layer bobot dengan parameter wajib shape yang merupakan tuple berisi ukuran matrix bobot\n",
    "Contoh: \n",
    "shape=(3, 4) berarti:\n",
    "- Untuk layer dengan 3 neuron awal dan layer dengan 4 neuron berikutnya\n",
    "- Menghasilkan matrix bobot dengan 4 kolom berdasarkan bias + neuron layer awal dikali 4 kolom berdasarkan neuron layer berikutnya\n",
    "\"\"\"\n",
    "class WeightInitializer:    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        w = np.zeros((shape[1], shape[0]))\n",
    "        b = np.zeros((shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(shape, lower_bound=-0.1, upper_bound=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        w = np.random.uniform(lower_bound, upper_bound, (shape[1], shape[0]))\n",
    "        b = np.random.uniform(lower_bound, upper_bound, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(shape, mean=0.0, variance=1.0, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        std_dev = np.sqrt(variance)  # Konversi variance ke standard deviation\n",
    "        w = np.random.normal(mean, std_dev, (shape[1], shape[0]))\n",
    "        b = np.random.normal(mean, std_dev, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def initialize_weights(initialization_type: str, shape, bias=1, lower_bound=-0.1, upper_bound=0.1, mean=0.0, variance=1.0, seed=None):\n",
    "    #     if initialization_type == 'zeros':\n",
    "    #         return WeightInitializer.zeros(shape, bias=bias)\n",
    "    #     elif initialization_type == 'uniform':\n",
    "    #         return WeightInitializer.uniform(shape, bias=bias, lower_bound=lower_bound, upper_bound=upper_bound, seed=seed)\n",
    "    #     elif initialization_type == 'normal':\n",
    "    #         return WeightInitializer.normal(shape, bias=bias, mean=mean, variance=variance, seed=seed)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis inisialisasi '{initialization_type}' tidak dikenal.\")\n",
    "    \n",
    "# Contoh penggunaan\n",
    "# zero_weights = WeightInitializer.zeros((3,4))\n",
    "# uniform_weights = WeightInitializer.uniform((3,4))\n",
    "# normal_weights = WeightInitializer.normal((3,4))\n",
    "# print(zero_weights)\n",
    "# print(uniform_weights)\n",
    "# print(normal_weights)\n",
    "# output:\n",
    "# [[0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]]\n",
    "# [[-0.00770413  0.05834501  0.00577898  0.01360891]\n",
    "#  [ 0.05610584  0.08511933 -0.08579279 -0.08257414]\n",
    "#  [-0.07634511 -0.09595632  0.06652397  0.05563135]\n",
    "#  [ 0.0279842   0.07400243  0.09572367  0.05983171]]\n",
    "# [[-0.88778575 -2.55298982  0.6536186   0.8644362 ]\n",
    "#  [-1.98079647 -0.74216502  2.26975462 -1.45436567]\n",
    "#  [-0.34791215  0.04575852 -0.18718385  1.53277921]\n",
    "#  [ 0.15634897  1.46935877  0.15494743  0.37816252]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencoba membuat FFNN \n",
    "\n",
    "# Yang menjadi ketentuan parameter FFNN:\n",
    "# - Jumlah layer\n",
    "# - Jumlah neuron tiap layer\n",
    "# - Fungsi aktivasi tiap layer\n",
    "# - Fungsi loss dari model\n",
    "# - Metode inisialisasi bobot\n",
    "\n",
    "# Method FFNN:\n",
    "# - Inisialisasi bobot\n",
    "# - Menyimpan bobot\n",
    "# - Menyimpan gradien bobot\n",
    "# - Menampilkan model struktur jaringan, bobot, dan gradien\n",
    "# - Menampilkan distribusi bobot\n",
    "# - Menampilkan distribusi gradien bobot\n",
    "# - Save and load\n",
    "# - Forward propagation\n",
    "# - Backward propagation\n",
    "# - Weight update dengan gradient descent\n",
    "\n",
    "# Parameter pelatihan FFNN:\n",
    "# - Batch size\n",
    "# - Learning rate\n",
    "# - Jumlah epoch\n",
    "# - Verbose\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self, layers, activations=None, loss=\"mse\", initialization=\"uniform\", seed=0, batch_size=1, learning_rate=0.01, epochs=10, verbose=1, weights=None):\n",
    "        # Parameter-parameter\n",
    "        # Menerima jumlah neuron dari setiap layer (sekaligus jumlah layernya) termasuk input dan output\n",
    "        self.layers = layers # Contoh: [1, 2, 3]\n",
    "        # Menerima fungsi aktivasi tiap layer\n",
    "        if activations:\n",
    "            self.activations = [ActivationFunction(activation) for activation in activations]\n",
    "        else:\n",
    "            self.activations = [ActivationFunction(\"sigmoid\") for _ in range(len(layers) - 1)]\n",
    "        # Menerima fungsi loss\n",
    "        self.loss = LossFunction(loss_type=loss)# Contoh: \"mse\"\n",
    "        # Menerima metode inisialisasi bobot\n",
    "        self.initialization = initialization # Contoh: \"zeros\"\n",
    "        self.seed = seed # Jika bobot bukan zeros, menerima seeding\n",
    "        self.batch_size = batch_size # Jumlah data yang diproses dalam satu iterasi\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs # Jumlah iterasi\n",
    "        self.verbose = verbose # 1 berarti menampilkan progress bar beserta kondisi training loss dan validation loss saat itu, jika 0 tidak usah\n",
    "        \n",
    "        # Inisialisasi bias dan bobot, beserta gradiennya\n",
    "        if self.initialization == 'custom':\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = []\n",
    "        self.gradients_w = []\n",
    "\n",
    "   \n",
    "        for i in range(1, len(self.layers)):\n",
    "            in_size, out_size = self.layers[i - 1], self.layers[i]\n",
    "            if self.initialization == 'zeros':\n",
    "                w = WeightInitializer.zeros((in_size, out_size))\n",
    "            elif self.initialization == 'uniform':\n",
    "                w = WeightInitializer.uniform((in_size, out_size), seed=self.seed)\n",
    "            elif self.initialization == 'normal':\n",
    "                w = WeightInitializer.normal((in_size, out_size), seed=self.seed)\n",
    "            elif self.initialization == 'custom':\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\"Metode inisialisasi tidak valid.\")\n",
    "            \n",
    "            self.weights.append(w)\n",
    "\n",
    "    # Saatnya forward propagation\n",
    "    def forward_propagation(self, input_data, target_output):\n",
    "        values = np.array(input_data)\n",
    "        value_matrix = [values]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            values = np.insert(values, 0, 1)  # Add bias term\n",
    "            z = np.dot(self.weights[i].T , values)\n",
    "            new_values = self.activations[i].forward(z) # Matrix dot multiplication antar weights di layer i dan values\n",
    "            values = new_values\n",
    "            value_matrix.append(values)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Forward Propagation: {values}\")\n",
    "            print(f\"Target: {target_output}\")\n",
    "            print(f\"Error: {self.loss.loss(y_pred=values, y_true=target_output)}\\n\")\n",
    "            \n",
    "        return value_matrix\n",
    "    \n",
    "    # def backward_propagation(self, target_output, value_matrix):\n",
    "    #     target_output = np.array(target_output)\n",
    "    #     gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        \n",
    "    #     # Compute output layer error\n",
    "    #     output_values = value_matrix[-1]\n",
    "\n",
    "    #     if self.loss == \"mse\":\n",
    "    #         delta = LossFunction.mse_derivative(y_pred=output_values, y_true=target_output) * ActivationFunctionDerivative.sigmoid(output_values)\n",
    "    #     elif self.loss == \"bce\":\n",
    "    #         delta = LossFunction.bce_derivative(y_pred=output_values, y_true=target_output) * ActivationFunctionDerivative.sigmoid(output_values)\n",
    "    #     elif self.loss == \"cce\":\n",
    "    #         delta = LossFunction.cce_derivative(y_pred=output_values, y_true=target_output) * ActivationFunctionDerivative.sigmoid(output_values)\n",
    "    #     else:\n",
    "    #         raise NotImplementedError(\"Loss function not implemented\")\n",
    "\n",
    "    #     # Backpropagate through layers\n",
    "    #     # TODO: need to debug and test this part to make sure the gradients are correct\n",
    "    #     for i in reversed(range(len(self.weights))):\n",
    "    #         delta = np.insert(delta, 0, 0) # bias doesnt update with this delta insert. Need to update value_matrix (?) with bias included if bias need updating\n",
    "    #         prev_values = np.insert(value_matrix[i], 0, 1)\n",
    "    #         gradients[i] = (delta * prev_values)\n",
    "\n",
    "    #         if i > 0:\n",
    "    #             delta = np.dot(self.weights[i].T, delta) * ActivationFunctionDerivative.sigmoid(value_matrix[i])\n",
    "    #     self.gradients_w = gradients\n",
    "    #     return gradients\n",
    "\n",
    "    def backward_propagation(self, target_output, value_matrix):\n",
    "        errors = [self.loss.loss_derivative(value_matrix[-1], target_output)]\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            delta = errors[-1] * self.activations[i].backward(value_matrix[i + 1])\n",
    "            prev_values = np.insert(value_matrix[i], 0, 1)  # Tambahkan bias\n",
    "            grad = np.outer(prev_values, delta)\n",
    "            self.gradients_w.insert(0, grad)\n",
    "            errors.append(np.dot(self.weights[i], delta)[1:])  # Hilangkan bias dari propagasi ke belakang\n",
    "\n",
    "    \n",
    "    def update_weights(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            if self.verbose:\n",
    "                print(f\"Layer {i}\")\n",
    "                print(f\"weights[{i}]: {self.weights[i]}\")\n",
    "                print(f\"gradients_w[{i}]: {self.gradients_w[i]}\\n\")\n",
    "            \n",
    "            self.weights[i] -= self.learning_rate * self.gradients_w[i]\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch}\")\n",
    "            total_loss = 0\n",
    "            for i in range(len(X)):\n",
    "                value_matrix = self.forward_propagation(X[i],y[i])\n",
    "                self.backward_propagation(y[i], value_matrix)\n",
    "                self.update_weights()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss.loss(y_pred=value_matrix[-1], y_true=y[i])\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(X)\n",
    "            if self.verbose and epoch % 1 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.5f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return [self.forward_propagation(x)[-1] for x in X]\n",
    "\n",
    "    # Untuk debugging\n",
    "    def debug(self):\n",
    "        return self.weights\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh: XOR problem\n",
    "# X = [[0, 0], [0, 1], [1,0], [1, 1]]\n",
    "# y = [[0], [1], [1], [0]]\n",
    "\n",
    "# ffnn = FFNN(layers=[2, 2, 1], initialization=\"zeros\", learning_rate=0.05, epochs=1, verbose=1)\n",
    "# ffnn.train(X, y)\n",
    "\n",
    "# prediction = ffnn.predict([[0,1]])\n",
    "# print(\"Prediction:\", prediction)\n",
    "\n",
    "X = [[0.05, 0.1]]\n",
    "y = [[0.01, 0.99]]\n",
    "\n",
    "weight = np.array([\n",
    "    [\n",
    "        [0.35, 0.35],\n",
    "        [0.15, 0.25],\n",
    "        [0.2, 0.3]\n",
    "    ],\n",
    "    [\n",
    "        [0.6, 0.6],\n",
    "        [0.4, 0.5],\n",
    "        [0.45, 0.55]\n",
    "    ]\n",
    "])\n",
    "\n",
    "ffnn = FFNN(layers=[2, 2, 2], loss=\"mse\", initialization\n",
    "=\"custom\", weights=weight, learning_rate=0.5, epochs=1, verbose=1)\n",
    "ffnn.train(X, y)\n",
    "print(ffnn.debug())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
