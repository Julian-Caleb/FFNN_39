{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar 1 IF3270 Pembelajaran Mesin <br /> Feedforward Neural Network\n",
    "\n",
    "## Kelompok 39\n",
    "\n",
    "- Dzaky Satrio Nugroho - 13522059\n",
    "- Julian Caleb Simandjuntak - 13522099\n",
    "- Rafiki Prawhira Harianto - 13522065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dulu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Aktivasi \n",
    "\n",
    "class ActivationFunction:\n",
    "    \n",
    "    # Fungsi linear\n",
    "    @staticmethod\n",
    "    def linear(x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    # Fungsi ReLU\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Fungsi Sigmoid\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Fungsi Hyperbolic Tangent\n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # Fungsi Softmax\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    # Fungsi Leaky ReLU\n",
    "    @staticmethod\n",
    "    def leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.maximum(alpha*x, x)\n",
    "\n",
    "    # Fungsi Swish\n",
    "    @staticmethod\n",
    "    def swish(x: np.ndarray) -> np.ndarray:\n",
    "        return x * ActivationFunction.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Loss\n",
    "\n",
    "class LossFunction:\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    @staticmethod\n",
    "    def mse(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        mse = np.sum((y_pred - y_true) ** 2) / len(y_true)\n",
    "        return mse\n",
    "\n",
    "    # Binary Cross-Entropy\n",
    "    @staticmethod\n",
    "    def bce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        bce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "        return bce\n",
    "\n",
    "    # Categorical Cross-Entropy\n",
    "    @staticmethod\n",
    "    def cce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        cce = -1 / len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
    "        return cce\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def calculate_loss(loss_type: str, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    #     if loss_type == 'mse':\n",
    "    #         return LossFunction.mse(y_pred, y_true)\n",
    "    #     elif loss_type == 'bce':\n",
    "    #         return LossFunction.bce(y_pred, y_true)\n",
    "    #     elif loss_type == 'cce':\n",
    "    #         return LossFunction.cce(y_pred, y_true)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis loss tidak dikenal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[-0.00770413  0.05834501  0.00577898  0.01360891]\n",
      " [ 0.05610584  0.08511933 -0.08579279 -0.08257414]\n",
      " [-0.07634511 -0.09595632  0.06652397  0.05563135]\n",
      " [ 0.0279842   0.07400243  0.09572367  0.05983171]]\n",
      "[[-0.88778575 -2.55298982  0.6536186   0.8644362 ]\n",
      " [-1.98079647 -0.74216502  2.26975462 -1.45436567]\n",
      " [-0.34791215  0.04575852 -0.18718385  1.53277921]\n",
      " [ 0.15634897  1.46935877  0.15494743  0.37816252]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Inisialisasi 1 layer bobot dengan parameter wajib shape yang merupakan tuple berisi ukuran matrix bobot\n",
    "Contoh: \n",
    "shape=(3, 4) berarti:\n",
    "- Untuk layer dengan 3 neuron awal dan layer dengan 4 neuron berikutnya\n",
    "- Menghasilkan matrix bobot dengan 4 kolom berdasarkan bias + neuron layer awal dikali 4 kolom berdasarkan neuron layer berikutnya\n",
    "\"\"\"\n",
    "class WeightInitializer:    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        w = np.zeros((shape[1], shape[0]))\n",
    "        b = np.zeros((shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(shape, lower_bound=-0.1, upper_bound=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        w = np.random.uniform(lower_bound, upper_bound, (shape[1], shape[0]))\n",
    "        b = np.random.uniform(lower_bound, upper_bound, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(shape, mean=0.0, variance=1.0, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        std_dev = np.sqrt(variance)  # Konversi variance ke standard deviation\n",
    "        w = np.random.normal(mean, std_dev, (shape[1], shape[0]))\n",
    "        b = np.random.normal(mean, std_dev, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def initialize_weights(initialization_type: str, shape, bias=1, lower_bound=-0.1, upper_bound=0.1, mean=0.0, variance=1.0, seed=None):\n",
    "    #     if initialization_type == 'zeros':\n",
    "    #         return WeightInitializer.zeros(shape, bias=bias)\n",
    "    #     elif initialization_type == 'uniform':\n",
    "    #         return WeightInitializer.uniform(shape, bias=bias, lower_bound=lower_bound, upper_bound=upper_bound, seed=seed)\n",
    "    #     elif initialization_type == 'normal':\n",
    "    #         return WeightInitializer.normal(shape, bias=bias, mean=mean, variance=variance, seed=seed)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis inisialisasi '{initialization_type}' tidak dikenal.\")\n",
    "    \n",
    "# Contoh penggunaan\n",
    "# zero_weights = WeightInitializer.zeros((3,4))\n",
    "# uniform_weights = WeightInitializer.uniform((3,4))\n",
    "# normal_weights = WeightInitializer.normal((3,4))\n",
    "# print(zero_weights)\n",
    "# print(uniform_weights)\n",
    "# print(normal_weights)\n",
    "# output:\n",
    "# [[0. 0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0. 0.]]\n",
    "# [[-0.01535893  0.01369034 -0.05323466  0.05658833 -0.0295206 ]\n",
    "#  [-0.05550326  0.06834675  0.03335891  0.03058884  0.05339302]\n",
    "#  [-0.07148145 -0.0300887  -0.07895016  0.05567438  0.03814023]]\n",
    "# [[ 0.25158888  1.34267031 -0.03036228 -1.16132763 -0.66723239]\n",
    "#  [ 0.64635165 -1.22173216 -0.41217616  0.9060914  -0.56040958]\n",
    "#  [-0.01703997 -0.64723922  0.89749136 -0.05013735 -1.27893224]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turunan Fungsi Aktivasi\n",
    "\n",
    "class ActivationFunctionDerivative:\n",
    "\n",
    "    # Fungsi Linear\n",
    "    @staticmethod\n",
    "    def linear(x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    # Fungsi RelU\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    # Fungsi Sigmoid\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.sigmoid(x)\n",
    "        return sigmoidx * (1 - sigmoidx)\n",
    "    \n",
    "    # Fungsi Hyperbolic Tangent\n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return (2 / (2 * np.sinh(x))) ** 2\n",
    "\n",
    "    # Fungsi Softmax\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        softmaxx = ActivationFunction.softmax(x)\n",
    "        n = x.size\n",
    "        matrix = []\n",
    "        for i in range(1,n+1):\n",
    "            row = []\n",
    "            for j in range(1,n+1):\n",
    "                row.append(softmaxx[i-1] * ((i == j) - softmaxx[j-1]))\n",
    "            matrix.append(row)\n",
    "\n",
    "        return np.array(matrix)\n",
    "\n",
    "    # Fungsi Leaky ReLU\n",
    "    @staticmethod\n",
    "    def leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    # Fungsi Swish\n",
    "    @staticmethod\n",
    "    def swish(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.sigmoid(x)\n",
    "        return sigmoidx * (1 + x - x * sigmoidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencoba membuat FFNN \n",
    "\n",
    "# Yang menjadi ketentuan parameter FFNN:\n",
    "# - Jumlah layer\n",
    "# - Jumlah neuron tiap layer\n",
    "# - Fungsi aktivasi tiap layer\n",
    "# - Fungsi loss dari model\n",
    "# - Metode inisialisasi bobot\n",
    "\n",
    "# Method FFNN:\n",
    "# - Inisialisasi bobot\n",
    "# - Menyimpan bobot\n",
    "# - Menyimpan gradien bobot\n",
    "# - Menampilkan model struktur jaringan, bobot, dan gradien\n",
    "# - Menampilkan distribusi bobot\n",
    "# - Menampilkan distribusi gradien bobot\n",
    "# - Save and load\n",
    "# - Forward propagation\n",
    "# - Backward propagation\n",
    "# - Weight update dengan gradient descent\n",
    "\n",
    "# Parameter pelatihan FFNN:\n",
    "# - Batch size\n",
    "# - Learning rate\n",
    "# - Jumlah epoch\n",
    "# - Verbose\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    # Untuk sementara, di edit manual, bukan input.\n",
    "    def __init__(self):\n",
    "        # Parameter-parameter\n",
    "        # Menerima jumlah neuron dari setiap layer (sekaligus jumlah layernya) termasuk input dan output\n",
    "        self.layers = [3, 4, 2, 4, 3, 2] # Contoh: [1, 2, 3]\n",
    "        # Menerima fungsi aktivasi tiap layer\n",
    "        self.activations = [\"sigmoid\", \"sigmoid\"] # Contoh: [\"sigmoid\", \"relu\"]\n",
    "        # Menerima fungsi loss\n",
    "        self.loss = \"mse\" # Contoh: \"mse\"\n",
    "        # Menerima metode inisialisasi bobot\n",
    "        self.initialization = \"uniform\" # Contoh: \"zeros\"\n",
    "        # Jika bobot bukan zeros, menerima seeding\n",
    "        self.seed = 0\n",
    "        \n",
    "        # Inisialisasi bias dan bobot, beserta gradiennya\n",
    "        self.weights = []\n",
    "        self.gradients_w = []\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            in_size, out_size = self.layers[i - 1], self.layers[i]\n",
    "            if self.initialization == 'zeros':\n",
    "                w = WeightInitializer.zeros((in_size, out_size))\n",
    "            elif self.initialization == 'uniform':\n",
    "                w = WeightInitializer.uniform((in_size, out_size), seed=self.seed)\n",
    "            elif self.initialization == 'normal':\n",
    "                w = WeightInitializer.normal((in_size, out_size), seed=self.seed)\n",
    "            else:\n",
    "                raise ValueError(\"Metode inisialisasi tidak valid.\")\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            \n",
    "    # Saatnya forward propagation\n",
    "    def forward_propagation(self, input_data):\n",
    "        data = input_data\n",
    "        for i in range(len(self.weights)):\n",
    "            print(\"Layer:\", i+1)\n",
    "            data = np.insert(data, 0, 1)\n",
    "            newdata = []\n",
    "            \n",
    "            for j in range(len(self.weights[i])):\n",
    "                print(\"Neuron:\", j+1)\n",
    "                z = np.dot(self.weights[i][j], data)\n",
    "                print(\"Hasil dot product:\", z)\n",
    "                result = ActivationFunction.sigmoid(z)\n",
    "                print(\"Hasil:\", result)\n",
    "                newdata.append(result)\n",
    "            \n",
    "            data = newdata\n",
    "            print(\"Hasil layer:\", data)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        # Untuk debugging\n",
    "        return data\n",
    "            \n",
    "        \n",
    "    # Untuk debugging\n",
    "    def debug(self):\n",
    "        return self.weights[0]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1\n",
      "Neuron: 1\n",
      "Hasil dot product: 0.07719946070759912\n",
      "Hasil: 0.5192902856775542\n",
      "Neuron: 2\n",
      "Hasil dot product: 0.09902911013964438\n",
      "Hasil: 0.5247370649571553\n",
      "Neuron: 3\n",
      "Hasil dot product: 0.08529436389619921\n",
      "Hasil: 0.5213106727167763\n",
      "Neuron: 4\n",
      "Hasil dot product: -0.01845014849257806\n",
      "Hasil: 0.49538759371777163\n",
      "Hasil layer: [0.5192902856775542, 0.5247370649571553, 0.5213106727167763, 0.49538759371777163]\n",
      "\n",
      "\n",
      "Layer: 2\n",
      "Neuron: 1\n",
      "Hasil dot product: 0.13554703843088167\n",
      "Hasil: 0.5338349713044834\n",
      "Neuron: 2\n",
      "Hasil dot product: 0.016379055541548233\n",
      "Hasil: 0.5040946723448031\n",
      "Hasil layer: [0.5338349713044834, 0.5040946723448031]\n",
      "\n",
      "\n",
      "Layer: 3\n",
      "Neuron: 1\n",
      "Hasil dot product: 0.11963938582058692\n",
      "Hasil: 0.5298742210254502\n",
      "Neuron: 2\n",
      "Hasil dot product: -0.007814884766250377\n",
      "Hasil: 0.498046288751584\n",
      "Neuron: 3\n",
      "Hasil dot product: 0.06490274904037707\n",
      "Hasil: 0.5162199939460769\n",
      "Neuron: 4\n",
      "Hasil dot product: 0.03861349458621478\n",
      "Hasil: 0.5096521743921069\n",
      "Hasil layer: [0.5298742210254502, 0.498046288751584, 0.5162199939460769, 0.5096521743921069]\n",
      "\n",
      "\n",
      "Layer: 4\n",
      "Neuron: 1\n",
      "Hasil dot product: 0.05540143298902488\n",
      "Hasil: 0.5138468167372012\n",
      "Neuron: 2\n",
      "Hasil dot product: 0.1250509076838921\n",
      "Hasil: 0.5312220506892288\n",
      "Neuron: 3\n",
      "Hasil dot product: -0.01520237213211154\n",
      "Hasil: 0.49619948016220605\n",
      "Hasil layer: [0.5138468167372012, 0.5312220506892288, 0.49619948016220605]\n",
      "\n",
      "\n",
      "Layer: 5\n",
      "Neuron: 1\n",
      "Hasil dot product: 0.025594869029374516\n",
      "Hasil: 0.5063983679650148\n",
      "Neuron: 2\n",
      "Hasil dot product: 0.08933448209940034\n",
      "Hasil: 0.522318779302184\n",
      "Hasil layer: [0.5063983679650148, 0.522318779302184]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5063983679650148, 0.522318779302184]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Run\n",
    "X = np.array([0, 1, 1])\n",
    "\n",
    "model = FFNN()\n",
    "model.forward_propagation(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
