{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar 1 IF3270 Pembelajaran Mesin <br /> Feedforward Neural Network\n",
    "\n",
    "## Kelompok 39\n",
    "\n",
    "- Dzaky Satrio Nugroho - 13522059\n",
    "- Julian Caleb Simandjuntak - 13522099\n",
    "- Rafiki Prawhira Harianto - 13522065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dulu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Aktivasi \n",
    "\n",
    "class ActivationFunction:\n",
    "    \n",
    "    # Fungsi linear\n",
    "    @staticmethod\n",
    "    def linear(x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    # Fungsi ReLU\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Fungsi Sigmoid\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Fungsi Hyperbolic Tangent\n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # Fungsi Softmax\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    # Fungsi Leaky ReLU\n",
    "    @staticmethod\n",
    "    def leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.maximum(alpha*x, x)\n",
    "\n",
    "    # Fungsi Swish\n",
    "    @staticmethod\n",
    "    def swish(x: np.ndarray) -> np.ndarray:\n",
    "        return x * ActivationFunction.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Loss\n",
    "\n",
    "class LossFunction:\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    @staticmethod\n",
    "    def mse(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        mse = np.sum((y_pred - y_true) ** 2) / len(y_true)\n",
    "        return mse\n",
    "\n",
    "    # Binary Cross-Entropy\n",
    "    @staticmethod\n",
    "    def bce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        bce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "        return bce\n",
    "\n",
    "    # Categorical Cross-Entropy\n",
    "    @staticmethod\n",
    "    def cce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        cce = -1 / len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
    "        return cce\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def calculate_loss(loss_type: str, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    #     if loss_type == 'mse':\n",
    "    #         return LossFunction.mse(y_pred, y_true)\n",
    "    #     elif loss_type == 'bce':\n",
    "    #         return LossFunction.bce(y_pred, y_true)\n",
    "    #     elif loss_type == 'cce':\n",
    "    #         return LossFunction.cce(y_pred, y_true)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis loss tidak dikenal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turunan Fungsi Aktivasi\n",
    "\n",
    "class ActivationFunctionDerivative:\n",
    "\n",
    "    # Fungsi Linear\n",
    "    @staticmethod\n",
    "    def linear(x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    # Fungsi RelU\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    # Fungsi Sigmoid\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.sigmoid(x)\n",
    "        return sigmoidx * (1 - sigmoidx)\n",
    "    \n",
    "    # Fungsi Hyperbolic Tangent\n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return (2 / (2 * np.sinh(x))) ** 2\n",
    "\n",
    "    # Fungsi Softmax\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        softmaxx = ActivationFunction.softmax(x)\n",
    "        n = x.size\n",
    "        matrix = []\n",
    "        for i in range(1,n+1):\n",
    "            row = []\n",
    "            for j in range(1,n+1):\n",
    "                row.append(softmaxx[i-1] * ((i == j) - softmaxx[j-1]))\n",
    "            matrix.append(row)\n",
    "\n",
    "        return np.array(matrix)\n",
    "\n",
    "    # Fungsi Leaky ReLU\n",
    "    @staticmethod\n",
    "    def leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    # Fungsi Swish\n",
    "    @staticmethod\n",
    "    def swish(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.sigmoid(x)\n",
    "        return sigmoidx * (1 + x - x * sigmoidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inisialisasi 1 layer bobot dengan parameter wajib shape yang merupakan tuple berisi ukuran matrix bobot\n",
    "Contoh: \n",
    "shape=(3, 4) berarti:\n",
    "- Untuk layer dengan 3 neuron awal dan layer dengan 4 neuron berikutnya\n",
    "- Menghasilkan matrix bobot dengan 4 kolom berdasarkan bias + neuron layer awal dikali 4 kolom berdasarkan neuron layer berikutnya\n",
    "\"\"\"\n",
    "class WeightInitializer:    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        w = np.zeros((shape[1], shape[0]))\n",
    "        b = np.zeros((shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(shape, lower_bound=-0.1, upper_bound=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        w = np.random.uniform(lower_bound, upper_bound, (shape[1], shape[0]))\n",
    "        b = np.random.uniform(lower_bound, upper_bound, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(shape, mean=0.0, variance=1.0, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        std_dev = np.sqrt(variance)  # Konversi variance ke standard deviation\n",
    "        w = np.random.normal(mean, std_dev, (shape[1], shape[0]))\n",
    "        b = np.random.normal(mean, std_dev, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def initialize_weights(initialization_type: str, shape, bias=1, lower_bound=-0.1, upper_bound=0.1, mean=0.0, variance=1.0, seed=None):\n",
    "    #     if initialization_type == 'zeros':\n",
    "    #         return WeightInitializer.zeros(shape, bias=bias)\n",
    "    #     elif initialization_type == 'uniform':\n",
    "    #         return WeightInitializer.uniform(shape, bias=bias, lower_bound=lower_bound, upper_bound=upper_bound, seed=seed)\n",
    "    #     elif initialization_type == 'normal':\n",
    "    #         return WeightInitializer.normal(shape, bias=bias, mean=mean, variance=variance, seed=seed)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis inisialisasi '{initialization_type}' tidak dikenal.\")\n",
    "    \n",
    "# Contoh penggunaan\n",
    "zero_weights = WeightInitializer.zeros((3,4))\n",
    "uniform_weights = WeightInitializer.uniform((3,4))\n",
    "normal_weights = WeightInitializer.normal((3,4))\n",
    "print(zero_weights)\n",
    "print(uniform_weights)\n",
    "print(normal_weights)\n",
    "# output:\n",
    "# [[0. 0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0. 0.]]\n",
    "# [[-0.01535893  0.01369034 -0.05323466  0.05658833 -0.0295206 ]\n",
    "#  [-0.05550326  0.06834675  0.03335891  0.03058884  0.05339302]\n",
    "#  [-0.07148145 -0.0300887  -0.07895016  0.05567438  0.03814023]]\n",
    "# [[ 0.25158888  1.34267031 -0.03036228 -1.16132763 -0.66723239]\n",
    "#  [ 0.64635165 -1.22173216 -0.41217616  0.9060914  -0.56040958]\n",
    "#  [-0.01703997 -0.64723922  0.89749136 -0.05013735 -1.27893224]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencoba membuat FFNN \n",
    "\n",
    "# Yang menjadi ketentuan parameter FFNN:\n",
    "# - Jumlah layer\n",
    "# - Jumlah neuron tiap layer\n",
    "# - Fungsi aktivasi tiap layer\n",
    "# - Fungsi loss dari model\n",
    "# - Metode inisialisasi bobot\n",
    "\n",
    "# Method FFNN:\n",
    "# - Inisialisasi bobot\n",
    "# - Menyimpan bobot\n",
    "# - Menyimpan gradien bobot\n",
    "# - Menampilkan model struktur jaringan, bobot, dan gradien\n",
    "# - Menampilkan distribusi bobot\n",
    "# - Menampilkan distribusi gradien bobot\n",
    "# - Save and load\n",
    "# - Forward propagation\n",
    "# - Backward propagation\n",
    "# - Weight update dengan gradient descent\n",
    "\n",
    "# Parameter pelatihan FFNN:\n",
    "# - Batch size\n",
    "# - Learning rate\n",
    "# - Jumlah epoch\n",
    "# - Verbose\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    # Untuk sementara, di edit manual, bukan input.\n",
    "    def __init__(self):\n",
    "        # Parameter-parameter\n",
    "        # Menerima jumlah neuron dari setiap layer (sekaligus jumlah layernya) termasuk input dan output\n",
    "        self.layers = [3, 4, 2] # Contoh: [1, 2, 3]\n",
    "        # Menerima fungsi aktivasi tiap layer\n",
    "        self.activations = [\"sigmoid\", \"sigmoid\"] # Contoh: [\"sigmoid\", \"relu\"]\n",
    "        # Menerima fungsi loss\n",
    "        self.loss = \"mse\" # Contoh: \"mse\"\n",
    "        # Menerima metode inisialisasi bobot\n",
    "        self.initialization = \"uniform\" # Contoh: \"zeros\"\n",
    "        # Jika bobot bukan zeros, menerima seeding\n",
    "        self.seed = 0\n",
    "        \n",
    "        # Inisialisasi bias dan bobot, beserta gradiennya\n",
    "        self.weights = []\n",
    "        self.gradients_w = []\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            in_size, out_size = self.layers[i - 1], self.layers[i]\n",
    "            if self.initialization == 'zeros':\n",
    "                w = WeightInitializer.zeros((in_size, out_size))\n",
    "            elif self.initialization == 'uniform':\n",
    "                w = WeightInitializer.uniform((in_size, out_size), seed=self.seed)\n",
    "            elif self.initialization == 'normal':\n",
    "                w = WeightInitializer.normal((in_size, out_size), seed=self.seed)\n",
    "            else:\n",
    "                raise ValueError(\"Metode inisialisasi tidak valid.\")\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            \n",
    "    # Saatnya forward propagation\n",
    "    def forward_propagation(self, input_data):\n",
    "        data = input_data\n",
    "        for i in range(len(self.weights)):\n",
    "            print(\"Layer:\", i+1)\n",
    "            data = np.insert(data, 0, 1)\n",
    "            newdata = []\n",
    "            \n",
    "            for j in range(len(self.weights[i])):\n",
    "                print(\"Neuron:\", j+1)\n",
    "                z = np.dot(self.weights[i][j], data)\n",
    "                print(\"Hasil dot product:\", z)\n",
    "                result = ActivationFunction.sigmoid(z)\n",
    "                print(\"Hasil:\", result)\n",
    "                newdata.append(result)\n",
    "            \n",
    "            data = newdata\n",
    "            print(\"Hasil layer:\", data)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        # Untuk debugging\n",
    "        return data\n",
    "            \n",
    "        \n",
    "    # Untuk debugging\n",
    "    def debug(self):\n",
    "        return self.weights[0]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Run\n",
    "# y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "model = FFNN()\n",
    "# weight = model.debug()\n",
    "# print(weight)\n",
    "# print(weights)\n",
    "\n",
    "X = np.array([0, 1, 1])\n",
    "data = X\n",
    "model.forward_propagation(data)\n",
    "\n",
    "    \n",
    "\n",
    "# for i in range(len(weight)) :\n",
    "#     print(i + 1)\n",
    "#     z = np.dot(weight[i], data)\n",
    "#     print(z)\n",
    "\n",
    "#     result = ActivationFunction.sigmoid(z)\n",
    "#     print(result)\n",
    "\n",
    "# y_pred = model.forward_propagation(X)\n",
    "# print(\"Predictions:\", y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
