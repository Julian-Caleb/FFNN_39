{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar 1 IF3270 Pembelajaran Mesin <br /> Feedforward Neural Network\n",
    "\n",
    "## Kelompok 39\n",
    "\n",
    "- Dzaky Satrio Nugroho - 13522059\n",
    "- Julian Caleb Simandjuntak - 13522099\n",
    "- Rafiki Prawhira Harianto - 13522065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dulu\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Aktivasi \n",
    "\n",
    "class ActivationFunction:\n",
    "    \n",
    "    # Fungsi linear\n",
    "    @staticmethod\n",
    "    def linear(x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    # Fungsi ReLU\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Fungsi Sigmoid\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Fungsi Hyperbolic Tangent\n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # Fungsi Softmax\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    # Fungsi Leaky ReLU\n",
    "    @staticmethod\n",
    "    def leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.maximum(alpha*x, x)\n",
    "\n",
    "    # Fungsi Swish\n",
    "    @staticmethod\n",
    "    def swish(x: np.ndarray) -> np.ndarray:\n",
    "        return x * ActivationFunction.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Loss\n",
    "\n",
    "class LossFunction:\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    @staticmethod\n",
    "    def mse(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        mse = np.sum((y_true - y_pred) ** 2) / len(y_true)\n",
    "        return mse\n",
    "\n",
    "    # Binary Cross-Entropy\n",
    "    @staticmethod\n",
    "    def bce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        bce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "        return bce\n",
    "\n",
    "    # Categorical Cross-Entropy\n",
    "    @staticmethod\n",
    "    def cce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        cce = -1 / len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
    "        return cce\n",
    "    \n",
    "    def mse_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -2 * (y_true - y_pred) / len(y_true) # times dy_pred/dw \n",
    "    \n",
    "    def bce_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -1 * (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true)) # times dy_pred/dw \n",
    "    \n",
    "    def cce_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -1 * (y_true / (y_pred * len(y_true))) # times dy_pred/dw \n",
    "    \n",
    "    # @staticmethod\n",
    "    # def calculate_loss(loss_type: str, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    #     if loss_type == 'mse':\n",
    "    #         return LossFunction.mse(y_pred, y_true)\n",
    "    #     elif loss_type == 'bce':\n",
    "    #         return LossFunction.bce(y_pred, y_true)\n",
    "    #     elif loss_type == 'cce':\n",
    "    #         return LossFunction.cce(y_pred, y_true)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis loss tidak dikenal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Inisialisasi 1 layer bobot dengan parameter wajib shape yang merupakan tuple berisi ukuran matrix bobot\n",
    "Contoh: \n",
    "shape=(3, 4) berarti:\n",
    "- Untuk layer dengan 3 neuron awal dan layer dengan 4 neuron berikutnya\n",
    "- Menghasilkan matrix bobot dengan 4 kolom berdasarkan bias + neuron layer awal dikali 4 kolom berdasarkan neuron layer berikutnya\n",
    "\"\"\"\n",
    "class WeightInitializer:    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        w = np.zeros((shape[1], shape[0]))\n",
    "        b = np.zeros((shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(shape, lower_bound=-0.1, upper_bound=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        w = np.random.uniform(lower_bound, upper_bound, (shape[1], shape[0]))\n",
    "        b = np.random.uniform(lower_bound, upper_bound, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(shape, mean=0.0, variance=1.0, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        std_dev = np.sqrt(variance)  # Konversi variance ke standard deviation\n",
    "        w = np.random.normal(mean, std_dev, (shape[1], shape[0]))\n",
    "        b = np.random.normal(mean, std_dev, (shape[1], 1))\n",
    "        return np.hstack((b, w))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def initialize_weights(initialization_type: str, shape, bias=1, lower_bound=-0.1, upper_bound=0.1, mean=0.0, variance=1.0, seed=None):\n",
    "    #     if initialization_type == 'zeros':\n",
    "    #         return WeightInitializer.zeros(shape, bias=bias)\n",
    "    #     elif initialization_type == 'uniform':\n",
    "    #         return WeightInitializer.uniform(shape, bias=bias, lower_bound=lower_bound, upper_bound=upper_bound, seed=seed)\n",
    "    #     elif initialization_type == 'normal':\n",
    "    #         return WeightInitializer.normal(shape, bias=bias, mean=mean, variance=variance, seed=seed)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis inisialisasi '{initialization_type}' tidak dikenal.\")\n",
    "    \n",
    "# Contoh penggunaan\n",
    "# zero_weights = WeightInitializer.zeros((3,4))\n",
    "# uniform_weights = WeightInitializer.uniform((3,4))\n",
    "# normal_weights = WeightInitializer.normal((3,4))\n",
    "# print(zero_weights)\n",
    "# print(uniform_weights)\n",
    "# print(normal_weights)\n",
    "# output:\n",
    "# [[0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]]\n",
    "# [[-0.00770413  0.05834501  0.00577898  0.01360891]\n",
    "#  [ 0.05610584  0.08511933 -0.08579279 -0.08257414]\n",
    "#  [-0.07634511 -0.09595632  0.06652397  0.05563135]\n",
    "#  [ 0.0279842   0.07400243  0.09572367  0.05983171]]\n",
    "# [[-0.88778575 -2.55298982  0.6536186   0.8644362 ]\n",
    "#  [-1.98079647 -0.74216502  2.26975462 -1.45436567]\n",
    "#  [-0.34791215  0.04575852 -0.18718385  1.53277921]\n",
    "#  [ 0.15634897  1.46935877  0.15494743  0.37816252]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turunan Fungsi Aktivasi\n",
    "\n",
    "class ActivationFunctionDerivative:\n",
    "\n",
    "    # Fungsi Linear\n",
    "    @staticmethod\n",
    "    def linear(x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    # Fungsi RelU\n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    # Fungsi Sigmoid\n",
    "    @staticmethod\n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    # Fungsi Hyperbolic Tangent\n",
    "    @staticmethod\n",
    "    def tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return (2 / (2 * np.sinh(x))) ** 2\n",
    "\n",
    "    # Fungsi Softmax\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        softmaxx = ActivationFunction.softmax(x)\n",
    "        n = x.size\n",
    "        matrix = []\n",
    "        for i in range(1,n+1):\n",
    "            row = []\n",
    "            for j in range(1,n+1):\n",
    "                row.append(softmaxx[i-1] * ((i == j) - softmaxx[j-1]))\n",
    "            matrix.append(row)\n",
    "\n",
    "        return np.array(matrix)\n",
    "\n",
    "    # Fungsi Leaky ReLU\n",
    "    @staticmethod\n",
    "    def leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    # Fungsi Swish\n",
    "    @staticmethod\n",
    "    def swish(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.sigmoid(x)\n",
    "        return sigmoidx * (1 + x - x * sigmoidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencoba membuat FFNN \n",
    "\n",
    "# Yang menjadi ketentuan parameter FFNN:\n",
    "# - Jumlah layer\n",
    "# - Jumlah neuron tiap layer\n",
    "# - Fungsi aktivasi tiap layer\n",
    "# - Fungsi loss dari model\n",
    "# - Metode inisialisasi bobot\n",
    "\n",
    "# Method FFNN:\n",
    "# - Inisialisasi bobot\n",
    "# - Menyimpan bobot\n",
    "# - Menyimpan gradien bobot\n",
    "# - Menampilkan model struktur jaringan, bobot, dan gradien\n",
    "# - Menampilkan distribusi bobot\n",
    "# - Menampilkan distribusi gradien bobot\n",
    "# - Save and load\n",
    "# - Forward propagation\n",
    "# - Backward propagation\n",
    "# - Weight update dengan gradient descent\n",
    "\n",
    "# Parameter pelatihan FFNN:\n",
    "# - Batch size\n",
    "# - Learning rate\n",
    "# - Jumlah epoch\n",
    "# - Verbose\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self, layers, activations=[\"sigmoid\", \"sigmoid\"], loss=\"mse\", initialization=\"uniform\", seed=0, batch_size=1, learning_rate=0.01, epochs=10, verbose=1, weights=None):\n",
    "        # Parameter-parameter\n",
    "        # Menerima jumlah neuron dari setiap layer (sekaligus jumlah layernya) termasuk input dan output\n",
    "        self.layers = layers # Contoh: [1, 2, 3]\n",
    "        # Menerima fungsi aktivasi tiap layer\n",
    "        self.activations = activations # Contoh: [\"sigmoid\", \"relu\"]\n",
    "        # Menerima fungsi loss\n",
    "        self.loss = loss # Contoh: \"mse\"\n",
    "        # Menerima metode inisialisasi bobot\n",
    "        self.initialization = initialization # Contoh: \"zeros\"\n",
    "        self.seed = seed # Jika bobot bukan zeros, menerima seeding\n",
    "        self.batch_size = batch_size # Jumlah data yang diproses dalam satu iterasi\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs # Jumlah iterasi\n",
    "        self.verbose = verbose # 1 berarti menampilkan progress bar beserta kondisi training loss dan validation loss saat itu, jika 0 tidak usah\n",
    "        \n",
    "        # Inisialisasi bias dan bobot, beserta gradiennya\n",
    "        if self.initialization == 'custom':\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = []\n",
    "        self.gradients_w = []\n",
    "\n",
    "   \n",
    "        for i in range(1, len(self.layers)):\n",
    "            in_size, out_size = self.layers[i - 1], self.layers[i]\n",
    "            if self.initialization == 'zeros':\n",
    "                w = WeightInitializer.zeros((in_size, out_size))\n",
    "            elif self.initialization == 'uniform':\n",
    "                w = WeightInitializer.uniform((in_size, out_size), seed=self.seed)\n",
    "            elif self.initialization == 'normal':\n",
    "                w = WeightInitializer.normal((in_size, out_size), seed=self.seed)\n",
    "            elif self.initialization == 'custom':\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\"Metode inisialisasi tidak valid.\")\n",
    "            \n",
    "            self.weights.append(w)\n",
    "\n",
    "    # Saatnya forward propagation\n",
    "    def forward_propagation(self, input_data, target_output):\n",
    "        values = np.array(input_data)\n",
    "        value_matrix = [values]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            values = np.insert(values, 0, 1)  # Add bias term\n",
    "            z = np.dot(self.weights[i].T , values)\n",
    "            new_values = ActivationFunction.sigmoid(z) # Matrix dot multiplication antar weights di layer i dan values\n",
    "            values = new_values\n",
    "            value_matrix.append(values)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Forward Propagation: {values}\")\n",
    "            print(f\"Target: {target_output}\")\n",
    "            print(f\"Error: {LossFunction.mse(y_pred=values, y_true=target_output)}\\n\")\n",
    "            \n",
    "        return value_matrix\n",
    "    \n",
    "    # def backward_propagation(self, target_output, value_matrix):\n",
    "    #     target_output = np.array(target_output)\n",
    "    #     gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        \n",
    "    #     # Compute output layer error\n",
    "    #     output_values = value_matrix[-1]\n",
    "    #     print(f\"output_values: {output_values}\")\n",
    "    #     print(f\"loss derivative: {LossFunction.mse_derivative(y_pred=output_values, y_true=target_output)}\")\n",
    "    #     print(f\"sigmoid: {ActivationFunctionDerivative.sigmoid(output_values)}\")\n",
    "\n",
    "    #     if self.loss == \"mse\":\n",
    "    #         delta = LossFunction.mse_derivative(y_pred=output_values, y_true=target_output) * ActivationFunctionDerivative.sigmoid(output_values)\n",
    "    #     elif self.loss == \"bce\":\n",
    "    #         delta = LossFunction.bce_derivative(y_pred=output_values, y_true=target_output) * ActivationFunctionDerivative.sigmoid(output_values)\n",
    "    #     elif self.loss == \"cce\":\n",
    "    #         delta = LossFunction.cce_derivative(y_pred=output_values, y_true=target_output) * ActivationFunctionDerivative.sigmoid(output_values)\n",
    "    #     else:\n",
    "    #         raise NotImplementedError(\"Loss function not implemented\")\n",
    "\n",
    "    #     # Backpropagate through layers\n",
    "    #     # TODO: need to debug and test this part to make sure the gradients are correct\n",
    "    #     for i in reversed(range(len(self.weights))):\n",
    "    #         delta = np.insert(delta, 0, 0) # bias doesnt update with this delta insert. Need to update value_matrix (?) with bias included if bias need updating\n",
    "    #         prev_values = np.insert(value_matrix[i], 0, 1)\n",
    "    #         gradients[i] = (delta * prev_values)\n",
    "    #         print(f\"gradient[{i}]: {gradients[i]}\")\n",
    "    #         print(f\"prev val\")\n",
    "\n",
    "    #         if i > 0:\n",
    "    #             print(f\"weights[{i}]: {self.weights[i]}\")\n",
    "    #             print(f\"delta: {delta}\")\n",
    "    #             delta = np.dot(self.weights[i].T, delta) * ActivationFunctionDerivative.sigmoid(value_matrix[i])\n",
    "    #     print(f\"weights[{i}]: {self.weights[i]}\")\n",
    "    #     print(f\"delta: {delta}\")\n",
    "    #     self.gradients_w = gradients\n",
    "    #     return gradients\n",
    "\n",
    "    def backward_propagation(self, y_true, value_matrix):\n",
    "        errors = [LossFunction.mse_derivative(value_matrix[-1], y_true)]\n",
    "        \n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            delta = errors[-1] * ActivationFunctionDerivative.sigmoid(value_matrix[i + 1])\n",
    "            prev_values = np.insert(value_matrix[i], 0, 1)  # Tambahkan bias\n",
    "            grad = np.outer(prev_values, delta)\n",
    "            self.gradients_w.insert(0, grad)\n",
    "            errors.append(np.dot(self.weights[i], delta)[1:])  # Hilangkan bias dari propagasi ke belakang\n",
    "    \n",
    "    def update_weights(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            if self.verbose:\n",
    "                print(f\"Layer {i}\")\n",
    "                print(f\"weights[{i}]: {self.weights[i]}\")\n",
    "                print(f\"gradients_w[{i}]: {self.gradients_w[i]}\\n\")\n",
    "            \n",
    "            # += or -=? Assume its -= until tested\n",
    "            self.weights[i] -= self.learning_rate * self.gradients_w[i]\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch}\")\n",
    "            total_loss = 0\n",
    "            for i in range(len(X)):\n",
    "                value_matrix = self.forward_propagation(X[i],y[i])\n",
    "                self.backward_propagation(y[i], value_matrix)\n",
    "                self.update_weights()\n",
    "\n",
    "                # Compute loss (MSE)\n",
    "                if self.loss == \"mse\":\n",
    "                    loss = LossFunction.mse(y_pred=value_matrix[-1], y_true=y[i])\n",
    "                elif self.loss == \"bce\":\n",
    "                    loss = LossFunction.bce(y_pred=value_matrix[-1], y_true=y[i])\n",
    "                elif self.loss == \"cce\":\n",
    "                    loss = LossFunction.cce(y_pred=value_matrix[-1], y_true=y[i])\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(X)\n",
    "            if self.verbose and epoch % 1 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.5f}\")\n",
    "\n",
    "        value_matrix = self.forward_propagation(X[i],y[i])\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return [self.forward_propagation(x)[-1] for x in X]\n",
    "\n",
    "    # Untuk debugging\n",
    "    def debug(self):\n",
    "        return self.weights\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.35, 0.35],\n",
      "       [0.15, 0.25],\n",
      "       [0.2 , 0.3 ]]), array([[0.6 , 0.6 ],\n",
      "       [0.4 , 0.5 ],\n",
      "       [0.45, 0.55]])]\n",
      "Epoch 0\n",
      "Forward Propagation: [0.75136507 0.77292847]\n",
      "Target: [0.01 0.99]\n",
      "Error: 0.2983711087600027\n",
      "\n",
      "Layer 0\n",
      "weights[0]: [[0.35 0.35]\n",
      " [0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "gradients_w[0]: [[0.00877135 0.00995425]\n",
      " [0.00043857 0.00049771]\n",
      " [0.00087714 0.00099543]]\n",
      "\n",
      "Layer 1\n",
      "weights[1]: [[0.6  0.6 ]\n",
      " [0.4  0.5 ]\n",
      " [0.45 0.55]]\n",
      "gradients_w[1]: [[ 0.13849856 -0.03809824]\n",
      " [ 0.08216704 -0.02260254]\n",
      " [ 0.08266763 -0.02274024]]\n",
      "\n",
      "Epoch 0, Loss: 0.29837\n",
      "Forward Propagation: [0.72844176 0.77837692]\n",
      "Target: [0.01 0.99]\n",
      "Error: 0.28047144679143016\n",
      "\n",
      "[array([[0.34561432, 0.34502287],\n",
      "       [0.14978072, 0.24975114],\n",
      "       [0.19956143, 0.29950229]]), array([[0.53075072, 0.61904912],\n",
      "       [0.35891648, 0.51130127],\n",
      "       [0.40866619, 0.56137012]])]\n"
     ]
    }
   ],
   "source": [
    "# Contoh: XOR problem\n",
    "# X = [[0, 0], [0, 1], [1,0], [1, 1]]\n",
    "# y = [[0], [1], [1], [0]]\n",
    "\n",
    "# ffnn = FFNN(layers=[2, 2, 1], initialization=\"zeros\", learning_rate=0.05, epochs=1, verbose=1)\n",
    "# ffnn.train(X, y)\n",
    "\n",
    "# prediction = ffnn.predict([[0,1]])\n",
    "# print(\"Prediction:\", prediction)\n",
    "\n",
    "X = [[0.05, 0.1]]\n",
    "y = [[0.01, 0.99]]\n",
    "\n",
    "weight = np.array([\n",
    "    [\n",
    "        [0.35, 0.35],\n",
    "        [0.15, 0.25],\n",
    "        [0.2, 0.3]\n",
    "    ],\n",
    "    [\n",
    "        [0.6, 0.6],\n",
    "        [0.4, 0.5],\n",
    "        [0.45, 0.55]\n",
    "    ]\n",
    "])\n",
    "\n",
    "ffnn = FFNN(layers=[2, 2, 2], activations=[\"sigmoid\", \"sigmoid\"], loss=\"mse\", initialization\n",
    "=\"custom\", weights=weight, learning_rate=0.5, epochs=1, verbose=1)\n",
    "ffnn.train(X, y)\n",
    "print(ffnn.debug())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
