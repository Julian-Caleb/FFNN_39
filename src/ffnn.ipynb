{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar 1 IF3270 Pembelajaran Mesin <br /> Feedforward Neural Network\n",
    "\n",
    "## Kelompok 39\n",
    "\n",
    "- Dzaky Satrio Nugroho - 13522059\n",
    "- Julian Caleb Simandjuntak - 13522099\n",
    "- Rafiki Prawhira Harianto - 13522065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dulu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Aktivasi \n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self, activation_type):\n",
    "        self.activation_type = activation_type\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self.activation_type == 'linear':\n",
    "            return ActivationFunction.__linear(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return ActivationFunction.__relu(x)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            return ActivationFunction.__sigmoid(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return ActivationFunction.__tanh(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return ActivationFunction.__softmax(x)\n",
    "        elif self.activation_type == 'leaky_relu':\n",
    "            return ActivationFunction.__leaky_relu(x)\n",
    "        elif self.activation_type == 'swish':\n",
    "            return ActivationFunction.__swish(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{self.activation_type}' not supported\")\n",
    "        \n",
    "    def backward(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self.activation_type == 'linear':\n",
    "            return ActivationFunction.__linear_derivative(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return ActivationFunction.__relu_derivative(x)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            return ActivationFunction.__sigmoid_derivative(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return ActivationFunction.__tanh_derivative(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return ActivationFunction.__softmax_derivative(x)\n",
    "        elif self.activation_type == 'leaky_relu':\n",
    "            return ActivationFunction.__leaky_relu_derivative(x)\n",
    "        elif self.activation_type == 'swish':\n",
    "            return ActivationFunction.__swish_derivative(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{self.activation_type}' not supported\")\n",
    "    \n",
    "    def __linear(x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    def __relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def __sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __tanh(x: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __softmax(x: np.ndarray) -> np.ndarray:\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def __leaky_relu(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.maximum(alpha*x, x)\n",
    "\n",
    "    def __swish(x: np.ndarray) -> np.ndarray:\n",
    "        return x * ActivationFunction.__sigmoid(x)\n",
    "\n",
    "    def __linear_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "    def __relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def __sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 - ActivationFunction.__tanh(x) ** 2\n",
    "\n",
    "    def __softmax_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        softmax_x = ActivationFunction.__softmax(x)\n",
    "        return softmax_x * (1 - softmax_x)\n",
    "\n",
    "    def __leaky_relu_derivative(x: np.ndarray, alpha=0.1) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "    def __swish_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        sigmoidx = ActivationFunction.__sigmoid(x)\n",
    "        return sigmoidx * (1 + x * (1 - sigmoidx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Loss\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, loss_type: str):\n",
    "        self.loss_type = loss_type\n",
    "    \n",
    "    def loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        if self.loss_type == 'mse':\n",
    "            return LossFunction.__mse(y_pred, y_true)\n",
    "        elif self.loss_type == 'bce':\n",
    "            return LossFunction.__bce(y_pred, y_true)\n",
    "        elif self.loss_type == 'cce':\n",
    "            return LossFunction.__cce(y_pred, y_true)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function {self.loss_type}\")\n",
    "        \n",
    "    def loss_derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        if self.loss_type == 'mse':\n",
    "            return LossFunction.__mse_derivative(y_pred, y_true)\n",
    "        elif self.loss_type == 'bce':\n",
    "            return LossFunction.__bce_derivative(y_pred, y_true)\n",
    "        elif self.loss_type == 'cce':\n",
    "            return LossFunction.__cce_derivative(y_pred, y_true)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function {self.loss_type}\")\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    def __mse(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        mse = np.sum((y_true - y_pred) ** 2) / len(y_true)\n",
    "        return mse\n",
    "\n",
    "    # Binary Cross-Entropy\n",
    "    def __bce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        bce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "        return bce\n",
    "\n",
    "    # Categorical Cross-Entropy\n",
    "    def __cce(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        cce = -1 / len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
    "        return cce\n",
    "    \n",
    "    def __mse_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -2 * (y_true - y_pred) / len(y_true) # times dy_pred/dw \n",
    "    \n",
    "    def __bce_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -1 * (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true)) # times dy_pred/dw \n",
    "    \n",
    "    def __cce_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return -1 * (y_true / (y_pred * len(y_true))) # times dy_pred/dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Inisialisasi Weight\n",
    "\n",
    "\"\"\"\n",
    "Inisialisasi 1 layer bobot dengan parameter wajib shape yang merupakan tuple berisi ukuran matrix bobot\n",
    "Contoh: \n",
    "shape=(3, 4) berarti:\n",
    "- Untuk layer dengan 3 neuron awal dan layer dengan 4 neuron berikutnya\n",
    "- Menghasilkan matrix bobot dengan 4 kolom berdasarkan bias + neuron layer awal dikali 4 kolom berdasarkan neuron layer berikutnya\n",
    "\"\"\"\n",
    "class WeightInitializer:    \n",
    "    @staticmethod\n",
    "    def zeros(shape):\n",
    "        w = np.zeros(shape)\n",
    "        b = np.zeros((1, shape[1]))\n",
    "        return np.vstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(shape, lower_bound=-0.1, upper_bound=0.1, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        w = np.random.uniform(lower_bound, upper_bound, shape)\n",
    "        b = np.random.uniform(lower_bound, upper_bound, (1, shape[1]))\n",
    "        return np.vstack((b, w))\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(shape, mean=0.0, variance=1.0, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        std_dev = np.sqrt(variance)  # Konversi variance ke standard deviation\n",
    "        w = np.random.normal(mean, std_dev, shape)\n",
    "        b = np.random.normal(mean, std_dev, (1, shape[1]))\n",
    "        return np.vstack((b, w))\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def initialize_weights(initialization_type: str, shape, bias=1, lower_bound=-0.1, upper_bound=0.1, mean=0.0, variance=1.0, seed=None):\n",
    "    #     if initialization_type == 'zeros':\n",
    "    #         return WeightInitializer.zeros(shape, bias=bias)\n",
    "    #     elif initialization_type == 'uniform':\n",
    "    #         return WeightInitializer.uniform(shape, bias=bias, lower_bound=lower_bound, upper_bound=upper_bound, seed=seed)\n",
    "    #     elif initialization_type == 'normal':\n",
    "    #         return WeightInitializer.normal(shape, bias=bias, mean=mean, variance=variance, seed=seed)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Jenis inisialisasi '{initialization_type}' tidak dikenal.\")\n",
    "    \n",
    "# Contoh penggunaan\n",
    "# zero_weights = WeightInitializer.zeros((3,4))\n",
    "# uniform_weights = WeightInitializer.uniform((3,4))\n",
    "# normal_weights = WeightInitializer.normal((3,4))\n",
    "# print(zero_weights)\n",
    "# print(uniform_weights)\n",
    "# print(normal_weights)\n",
    "# output:\n",
    "# [[0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]\n",
    "#  [0. 0. 0. 0.]]\n",
    "# [[-0.00770413  0.05834501  0.00577898  0.01360891]\n",
    "#  [ 0.05610584  0.08511933 -0.08579279 -0.08257414]\n",
    "#  [-0.07634511 -0.09595632  0.06652397  0.05563135]\n",
    "#  [ 0.0279842   0.07400243  0.09572367  0.05983171]]\n",
    "# [[-0.88778575 -2.55298982  0.6536186   0.8644362 ]\n",
    "#  [-1.98079647 -0.74216502  2.26975462 -1.45436567]\n",
    "#  [-0.34791215  0.04575852 -0.18718385  1.53277921]\n",
    "#  [ 0.15634897  1.46935877  0.15494743  0.37816252]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mencoba membuat FFNN \n",
    "\n",
    "# Yang menjadi ketentuan parameter FFNN:\n",
    "# - Jumlah layer\n",
    "# - Jumlah neuron tiap layer\n",
    "# - Fungsi aktivasi tiap layer\n",
    "# - Fungsi loss dari model\n",
    "# - Metode inisialisasi bobot\n",
    "\n",
    "# Method FFNN:\n",
    "# - Inisialisasi bobot\n",
    "# - Menyimpan bobot\n",
    "# - Menyimpan gradien bobot\n",
    "# - Menampilkan model struktur jaringan, bobot, dan gradien\n",
    "# - Menampilkan distribusi bobot\n",
    "# - Menampilkan distribusi gradien bobot\n",
    "# - Save and load\n",
    "# - Forward propagation\n",
    "# - Backward propagation\n",
    "# - Weight update dengan gradient descent\n",
    "\n",
    "# Parameter pelatihan FFNN:\n",
    "# - Batch size\n",
    "# - Learning rate\n",
    "# - Jumlah epoch\n",
    "# - Verbose\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    def __init__(self, layers, activations=None, loss=\"mse\", initialization=\"uniform\", seed=39, batch_size=1, learning_rate=0.01, epochs=10, verbose=1, weights=None, w_lower_bound=-0.1, w_upper_bound=0.1, w_mean=1.0, w_variance=0.0):\n",
    "        # Parameter-parameter\n",
    "        # Menerima jumlah neuron dari setiap layer (sekaligus jumlah layernya) termasuk input dan output\n",
    "        self.layers = layers # Contoh: [1, 2, 3]\n",
    "        # Menerima fungsi aktivasi tiap layer\n",
    "        if activations:\n",
    "            self.activations = [ActivationFunction(activation) for activation in activations]\n",
    "        else:\n",
    "            self.activations = [ActivationFunction(\"sigmoid\") for _ in range(len(layers) - 1)]\n",
    "        # Menerima fungsi loss\n",
    "        self.loss = LossFunction(loss_type=loss)# Contoh: \"mse\"\n",
    "        # Menerima metode inisialisasi bobot\n",
    "        self.initialization = initialization # Contoh: \"zeros\"\n",
    "        self.seed = seed # Jika bobot bukan zeros, menerima seeding\n",
    "        self.batch_size = batch_size # Jumlah data yang diproses dalam satu iterasi\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs # Jumlah iterasi\n",
    "        self.verbose = verbose # 1 berarti menampilkan progress bar beserta kondisi training loss dan validation loss saat itu, jika 0 tidak usah\n",
    "        self.value_matrix = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        # Inisialisasi bias dan bobot, beserta gradiennya\n",
    "        if self.initialization == 'custom':\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = []\n",
    "        self.gradients_w = []\n",
    "\n",
    "   \n",
    "        for i in range(1, len(self.layers)):\n",
    "            in_size, out_size = self.layers[i - 1], self.layers[i]\n",
    "            if self.initialization == 'zeros':\n",
    "                w = WeightInitializer.zeros((in_size, out_size))\n",
    "            elif self.initialization == 'uniform':\n",
    "                w = WeightInitializer.uniform((in_size, out_size), seed=self.seed, lower_bound=w_lower_bound, upper_bound=w_upper_bound)\n",
    "            elif self.initialization == 'normal':\n",
    "                w = WeightInitializer.normal((in_size, out_size), seed=self.seed, mean=w_mean, variance=w_variance)\n",
    "            elif self.initialization == 'custom':\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\"Metode inisialisasi tidak valid.\")\n",
    "            \n",
    "            self.weights.append(w)\n",
    "\n",
    "    # Saatnya forward propagation\n",
    "    def forward_propagation(self, input_data):\n",
    "        values = np.array(input_data)\n",
    "        self.value_matrix = [values]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            values = np.insert(values, 0, 1)  # Add bias term\n",
    "            z = np.dot(self.weights[i].T , values)\n",
    "            new_values = self.activations[i].forward(z) # Matrix dot multiplication antar weights di layer i dan values\n",
    "            values = new_values\n",
    "            self.value_matrix.append(values)\n",
    "        \n",
    "        return self.value_matrix[-1] # Print hasil\n",
    "\n",
    "    def backward_propagation(self, target_output):\n",
    "        errors = [self.loss.loss_derivative(self.value_matrix[-1], target_output)]\n",
    "        self.gradients_w = []\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            delta = errors[-1] * self.activations[i].backward(self.value_matrix[i + 1])\n",
    "            prev_values = np.insert(self.value_matrix[i], 0, 1)  # Tambahkan bias\n",
    "            grad = np.outer(prev_values, delta)\n",
    "            self.gradients_w.insert(0, grad)\n",
    "            errors.append(np.dot(self.weights[i], delta)[1:])  # Hilangkan bias dari propagasi ke belakang\n",
    "\n",
    "    \n",
    "    def update_weights(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            # if self.verbose:\n",
    "            #     print(f\"Layer {i}\")\n",
    "            #     print(f\"weights[{i}]: {self.weights[i]}\")\n",
    "            #     print(f\"gradients_w[{i}]: {self.gradients_w[i]}\\n\")\n",
    "            \n",
    "            self.weights[i] -= self.learning_rate * self.gradients_w[i] / self.batch_size\n",
    "\n",
    "    def train(self, X, y, val_split=0.2):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Split data\n",
    "        num_samples = len(X)\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X, y = X[indices], y[indices]\n",
    "        if num_samples < 2: # in case train sample is 1\n",
    "            X_train, y_train = X, y  \n",
    "            X_val, y_val = np.array([]), np.array([])  \n",
    "        else:\n",
    "            split_index = int((1 - val_split) * num_samples)\n",
    "            X_train, y_train = X[:split_index], y[:split_index]\n",
    "            X_val, y_val = X[split_index:], y[split_index:]\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # if self.verbose:\n",
    "            #     print(f\"Epoch {epoch}\")\n",
    "            total_loss = 0\n",
    "\n",
    "            num_samples_train = len(X_train)\n",
    "            indices = np.arange(num_samples_train)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "            \n",
    "            for i in range(0, num_samples_train, self.batch_size):\n",
    "                batch_X = X_train[i:i + self.batch_size]\n",
    "                batch_y = y_train[i:i + self.batch_size]\n",
    "                batch_gradients = [np.zeros_like(w) for w in self.weights]\n",
    "                batch_loss = 0\n",
    "                \n",
    "                for j in range(len(batch_X)):\n",
    "                    self.forward_propagation(batch_X[j])\n",
    "                    self.backward_propagation(batch_y[j])\n",
    "                    batch_loss += self.loss.loss(y_pred=self.value_matrix[-1], y_true=batch_y[j])\n",
    "                    for k in range(len(self.weights)):\n",
    "                        batch_gradients[k] += self.gradients_w[k]\n",
    "                \n",
    "                self.gradients_w = batch_gradients\n",
    "                self.update_weights()\n",
    "                total_loss += batch_loss / len(batch_X)\n",
    "            \n",
    "            avg_loss = total_loss / max(num_samples_train / self.batch_size, 1)\n",
    "            self.train_losses.append(avg_loss)\n",
    "            \n",
    "            # Menghitung validation loss\n",
    "            val_loss = 0\n",
    "            for i in range(len(X_val)):\n",
    "                self.forward_propagation(X_val[i])\n",
    "                val_loss += self.loss.loss(y_pred=self.value_matrix[-1], y_true=y_val[i])\n",
    "            avg_val_loss = val_loss / max(len(X_val), 1)\n",
    "            self.val_losses.append(avg_val_loss)\n",
    "\n",
    "            if self.verbose and epoch % 1 == 0:\n",
    "                print(\"Progress: [\", end=\"\")\n",
    "                for i in range (epoch) :\n",
    "                    print(\"#\", end=\"\")\n",
    "                for i in range(self.epochs - epoch):\n",
    "                    print(\"-\", end=\"\")\n",
    "                print(f\"] Epoch {epoch}/{self.epochs}\")\n",
    "                    \n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.5f}, Validation Loss: {avg_val_loss:.5f}\") \n",
    "                \n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, self.epochs + 1), self.train_losses, label='Training Loss', color='blue')\n",
    "        plt.plot(range(1, self.epochs + 1), self.val_losses, label='Validation Loss', color='black')\n",
    "        plt.title('Loss VS Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()          \n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            x_pred = self.forward_propagation(x)\n",
    "            predictions.append(x_pred)\n",
    "        return predictions\n",
    "        \n",
    "    def visualize_weights(self, start=1, end=None, display_size=5, gradient_graph=False):\n",
    "        if end is None:\n",
    "            end = len(self.value_matrix)\n",
    "        max_nodes_in_layer = 0\n",
    "        \n",
    "        for i in range(len(self.value_matrix)):\n",
    "            if len(self.value_matrix[i]) > max_nodes_in_layer:\n",
    "                max_nodes_in_layer = len(self.value_matrix[i])\n",
    "\n",
    "        number_of_input = len(self.value_matrix[0])\n",
    "        number_of_hidden_layer = len(self.value_matrix) - 2\n",
    "        number_of_output = len(self.value_matrix[len(self.value_matrix)-1])\n",
    "\n",
    "        nodes = {}\n",
    "\n",
    "        # Algoritma Nodes\n",
    "        input_nodes = []\n",
    "        for i in range(number_of_input):\n",
    "            input_nodes.append('i' + str(i+1))\n",
    "        input_nodes.append('b1')\n",
    "\n",
    "        index_to_layer = {}\n",
    "        index = 1\n",
    "\n",
    "        index_to_layer[index] = 'input'\n",
    "        nodes['input'] = input_nodes\n",
    "        index += 1\n",
    "\n",
    "        hidden_nodes = []\n",
    "        for i in range(number_of_hidden_layer):\n",
    "            hidden_nodes = []\n",
    "            for j in range(len(self.weights[i][0])):\n",
    "                hidden_nodes.append('h' + str(i+1) + '-' + str(j+1))\n",
    "            hidden_nodes.append('b' + str(i+2))\n",
    "            nodes['hidden' + str(i+1)] = hidden_nodes\n",
    "            index_to_layer[index] = 'hidden' + str(i+1)\n",
    "            index += 1\n",
    "\n",
    "        output_nodes = []\n",
    "        for i in range(number_of_output):\n",
    "            output_nodes.append('o' + str(i+1))\n",
    "\n",
    "        nodes['output'] = output_nodes\n",
    "        index_to_layer[index] = 'output'\n",
    "\n",
    "        edges = []\n",
    "        edge_labels = {}\n",
    "        partial_nodes = {}\n",
    "        max_height = 0\n",
    "\n",
    "        partial_nodes[index_to_layer[start]] = nodes[index_to_layer[start]]\n",
    "\n",
    "        # Algoritma label\n",
    "        for layer_idx in range(start - 1, end - 1):\n",
    "            if len(nodes[index_to_layer[layer_idx+1]]) > max_height:\n",
    "                max_height = len(nodes[index_to_layer[layer_idx+1]])\n",
    "\n",
    "            prev_layer = index_to_layer[layer_idx + 1]\n",
    "            next_layer = index_to_layer[layer_idx + 2]\n",
    "            \n",
    "            partial_nodes[next_layer] = nodes[next_layer]  # Add next layer\n",
    "            \n",
    "            weight_matrix = self.gradients_w[layer_idx] if gradient_graph else self.weights[layer_idx]\n",
    "            \n",
    "            for prev_idx, prev_neuron in enumerate(nodes[prev_layer]):  \n",
    "                for next_idx, next_neuron in enumerate(nodes[next_layer]):  \n",
    "                    if next_neuron.startswith('b'):  # Ignore bias connection\n",
    "                        continue  \n",
    "                    \n",
    "                    edges.append((prev_neuron, next_neuron))  \n",
    "                    \n",
    "                    # Bias in first row\n",
    "                    weight_value = weight_matrix[0][next_idx] if prev_neuron.startswith('b') else weight_matrix[prev_idx + 1][next_idx]  \n",
    "                    edge_labels[(prev_neuron, next_neuron)] = str(weight_value)[:display_size]  \n",
    "\n",
    "        print(max_height)\n",
    "        # Graph\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(sum(partial_nodes.values(), []))\n",
    "        G.add_edges_from(edges)\n",
    "\n",
    "        # Position nodes\n",
    "        pos = {}\n",
    "\n",
    "        # Horizontal offset untuk tiap kolom\n",
    "        x_offset = {}\n",
    "        for i in range(len(self.value_matrix)):\n",
    "            x_offset[i] = i+1\n",
    "\n",
    "        y_positions = {}\n",
    "\n",
    "        # Algoritma penentuan lokasi nodes\n",
    "        iterator = 0\n",
    "        for i in range(start-1, end):\n",
    "            if i == len(self.value_matrix) - 1:\n",
    "                y_positions[iterator] = [k for k in range(-1, -len(self.value_matrix[i]) - 1, -1)]\n",
    "            else:\n",
    "                y_positions[iterator] = [k for k in range(-1, -len(self.value_matrix[i]) - 2, -1)]\n",
    "            iterator += 1\n",
    "\n",
    "        for i, (_, nodes_list) in enumerate(partial_nodes.items()):\n",
    "            for j, node in enumerate(nodes_list):\n",
    "                pos[node] = (x_offset[i], y_positions[i][j])\n",
    "\n",
    "        # Draw graph\n",
    "        node_color = []\n",
    "        for node in G.nodes():\n",
    "            if node.startswith('b'): # Bias\n",
    "                node_color.append('lightblue')\n",
    "            elif node.startswith('i'): # Input layer\n",
    "                node_color.append('lightgreen') \n",
    "            elif node.startswith('h'): # Hidden layer\n",
    "                node_color.append('skyblue')\n",
    "            elif node.startswith('o'): # Output layer\n",
    "                node_color.append('orange')\n",
    "\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=800, edgecolors='black')\n",
    "        nx.draw_networkx_edges(G, pos, arrows=True, arrowstyle='->', width=1.5)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
    "\n",
    "        # Draw edge labels\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G,\n",
    "            pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_size=8,\n",
    "            font_color='red',\n",
    "            node_size=5500,\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7),\n",
    "            label_pos=0  # Ensure labels are closer to the source node\n",
    "        )\n",
    "\n",
    "        # Menambah title pada gambar\n",
    "        plt.title(\"Visualisasi Weight\")\n",
    "        # Set lebar gambar\n",
    "        plt.xlim(0, end - start + 2)\n",
    "        # Set tinggi gambar\n",
    "        plt.ylim(max_height * (-1) - 2, 0)\n",
    "        # Menghilangkan garis axis\n",
    "        plt.axis('off')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_gradient_weights(self, start=1, end=None, display_size=5):\n",
    "        self.visualize_weights(start=start, end=end, display_size=display_size, gradient_graph=True)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model loaded from {file_path}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOKAN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasetnya dulu min\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Konvert dulu karena aselinya string\n",
    "y_int = y.astype(int).reshape(-1, 1) \n",
    "\n",
    "# Kita one hot encoding sejenak\n",
    "encoder = OneHotEncoder(sparse_output=False) \n",
    "y_one_hot = encoder.fit_transform(y_int)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bikin modelnya gan\n",
    "model = FFNN(\n",
    "    layers = [784, 3, 3, 10], # Input harus 784, output harus 10\n",
    "    activations = [\"sigmoid\", \"sigmoid\", \"sigmoid\"],\n",
    "    loss = \"mse\",\n",
    "    initialization = \"zeros\",\n",
    "    learning_rate = 0.01,\n",
    "    epochs = 3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Kita train sejenak\n",
    "model.train(X_train, y_train, val_split=0.2)\n",
    "# model.plot_loss()\n",
    "# model.visualize_weights(start=1, end=None, display_size=5)\n",
    "# model.visualize_gradient_weights(start=1, end=None, display_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi skor gan\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "result = np.zeros_like(y_pred)\n",
    "for i, row in enumerate(y_pred):\n",
    "    max_index = np.argmax(row)\n",
    "    result[i, max_index] = 1\n",
    "y_pred = result\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "correct_predictions = np.sum(y_pred_classes == y_test_classes)\n",
    "total_samples = y_test.shape[0]\n",
    "    \n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Akurasi: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
